---
title: "Statistical Models/Tests Part1"
author: "Shannon B. Hagerty"
date: "6/24/2019"
output:
  pdf_document: default
  html_document: default
---
Welcome to this week's SummeR of R session, our first of two sessions learning about statistical models/tests in R! 

### Today we'll cover:

- Checking for normality and equal variance 
- T-tests and Anovas 

### Set up
To get started let's load or install the library's we'll be using. If this is your first time using one of the packages uncomment and run the appropriate install.package('package')
```{r}
#install.packages('tidyverse')
library(tidyverse)

#install.packages('car')
library(car)

#install.packages('broom')
library(broom)

```

We're going to be using some wine_ratings data set from the Tidy Tuesday repository.Let's load that in now. 

```{r}
wine_ratings <- readr::read_csv("https://raw.githubusercontent.com/rfordatascience/tidytuesday/master/data/2019/2019-05-28/winemag-data-130k-v2.csv")
```


## Could I be a wine rater? A one sample t-test

I would like to be a wine rater but I think that most wine raters are probably tasting more expensive wine than any I buy. Let's say that I am willing to spend on average $25 for a bottle of wine (not really). Would I be comparable to other reviewers then? I am going to run a one sample t-test to see if that average cost would be different from the average for the whole group of reviewers. 

Below I am going to reorganize my data so that I have the average price for each reviewer. 

```{r}
reviewers<-wine_ratings %>% group_by(taster_name) %>% summarize(avg_price = mean(price, na.rm=TRUE))
```

I'm going to plot my data now to check to see if it's normally distributed and to make sure there are no outliers. 
```{r}
# A histogram
ggplot(reviewers, aes(x=avg_price))+
  geom_histogram(bins=7)

#A qqplot
ggplot(reviewers, aes(sample=avg_price))+
  geom_qq()
```
The qqplot suggests the data are normally distributed, but let's do a Shapiro-Wilks test to find out. The *shapiro.test()* function is part of the stats package which is included in base R so you don't need to load or install it.  If you take a look at the documentation, you'll see its input is a vector of data values. Each column in our data frame is a vector. When we want to call a specific column in R, we call the dataframe name and then follow it with a dollar sign then type the column name (see below reviewer$avg_price) this is how we index specific columns in R. 

```{r}
shapiro.test(reviewers$avg_price)
```
Okay, so our p-value suggests we fail to reject null that the data are normally distributed. We'll go on using the parametric t-test. 

So is the average for all the reviewers significantly different than my proposed average of $25? 
```{r}
t.test(reviewers$avg_price, mu=25)
```

We see that my proposed average cost per bottle would be significantly different from  the reviewers rating.  But I actually don't just care about if its different or not, I care about the direction of the difference.  I don't want to be rating on average cheaper wine. So I want to test if the mean of the population is significantly greater than my proposed cost.   
```{r}
t.test(reviewers$avg_price, mu=25, alternative = "greater")
```
## Am I missing out? Comparing two means: expensive and affordable wines  

So the average reviewer would spend more on a bottle of wine than I would be willing to. But does that really mean the yare tasting better wines than I would be?  
  
Lets sort the wines into two categories based on their affordability. Then I can compare the wines in each group to see if they actually score better.  
```{r}
wine_ratings<-wine_ratings %>% mutate(affordability = factor(case_when(price > 25 ~'expensive', TRUE ~ 'affordable')))
```

You might see something different in that bit of code.  I put the *case_when()* function inside the *factor()* function.  Factors can be handy when you have categorical variables. They are a computationally more efficient way to work with your variables. Also now when you get a summary of your dataframe you get the count of each factor in your dataframe.  
```{r}
summary(wine_ratings$affordability)
```
We have so many observations, I'm not going to worry about assumptions much.  But for good measure let's plot the two groups to make sure there's nothing weird going on.   

```{r}
ggplot(wine_ratings)+
  geom_histogram(aes(x=points), bins=20)+ #default is 30 bins, you can play around with this a bit
  facet_grid(affordability~.)+
  theme_classic()
```
We can check to see if the two samples have equal variance by performing a leveneTest, inside the functions we put a formula of the structure 'continuous_variable ~ grouping' our continuous variable is the points value and the samples are grouped by their category in the affordability column.

```{r}
leveneTest(points~affordability, data=wine_ratings)
```
Levene's test has a null hypothesis that the variances are equal,based on the results of our test we would reject the null and assume unequal variance.  

We need the data to be in vectors (i.e. a list of elements all of the same data type) in order to perform the t-test. To do the two sample t-test we actually need one vector with the points data for the affordable wines and then another vector of the points data for the expensive wines.  We can use filter to grab just the rows that are affordable and then we use pull to just grab the points values for those rows. Pull will return the points values as a vector (unlike *select()* which would give them in the form of a dataframe). We do the same thing for the expensive wines.
```{r}
affordable<-filter(wine_ratings, affordability == 'affordable') %>% pull(points)
expensive <- filter(wine_ratings, affordability == 'expensive') %>% pull(points)
```
Now that we have the data in the form of two vectors we can do another variance test using *var.test()* which uses and F test, where the null hypothesis is that the ratio of the two variances is 1. 
```{r}
var.test(affordable, expensive)
```
We also reject the null hypothesis here and we'll assume unequal variances for the t-test.  

Using this dataset I found an interesting limit to the *shapiro.test()*: 
```{r}
#shapiro.test(affordable)
#shapiro.test(expensive)
```
It has a 5000 sample maximum, at that point you may want to look at a qqplot to see if the sample deviates from normal in a way that is meaningful for your analysis.
  
We'll use the *t.test()* to compare the two means, the default for the function is to assume variances are not equal (which works here), and it performs a Welch Two Sample t-test. If you had equal variance you would set the argument var.equal=TRUE which would run a Two-sample t-test.  

```{r}
t.test(affordable, expensive) #if variances were equal we would have added the argument var.equal=TRUE inside the function. 
```
The two samples are significantly different from each other, the affordable wines have just slightly lower scores than the expensive wines.  
  
We can see the two samples below:

```{r}
ggplot(wine_ratings)+
  geom_boxplot(aes(x=affordability, y=points, fill=affordability))+
  theme_classic()+
  ylab('Wine Score')+
  xlab('')+
  theme(legend.position = 'none')
```
**Note on paired t-tests**  
If you wanted to run a paired t-test you would have the two vectors and specify the argumend *paired=TRUE* inside the t.test() function

**Note on wilcoxon test**
If the data had not been normally distributed and you could not transform it to be normal, then you could run the wilcoxon / mann-whitney test with the function *wilcox.test()* inside you have one vector of data and either another vector or a mean value you want to compare your sample to, you can also specify an argument alternative = 'greater' or 'less'

## Comparing means from multiple groups

Can I somehow select wines that give me the best bang for my buck (most points/price)? Would it make sense to choose wines only from certain countries?  Let's if country of origin signficantly affects the ratio of points/price.  
  
First we'll narrow down the dataset a bit, to include only the 5 countries with most wine reviews and clean the data a little bit. 

```{r}
#grab the 5 most reviewed countries
country_df<-wine_ratings %>% group_by(country) %>% summarise(count=n()) %>% top_n(5)
#use that to filter-join the wine_ratings list and select only columns I'm going to be interested in. 
country_df<-semi_join(wine_ratings, country_df)%>% select(country, price, points, variety)%>%drop_na()
#we'll turn country into a factor
country_df<-mutate(country_df, country=factor(country))

```

We can use the *aov()* function to run our anova.  We'll check our model assumptions after we fit the model. You want to assign the model fit to a variable name because we'll use it to get the statistics and check assumptions. 
  
  
```{r}
anova<-aov(points~country, data=country_df)
```
You can use the functions we used earlier to test model assumptions (e.g. shapiro.test(), leveneTest()). You can also check them out by using the base R *plot()* and putting variable name you assigned the model fit to inside of it. This will give you four plots that can let you interpet if residuals meet assumptions for heteroscedacity and normality. 
  
```{r}
plot(anova)
```
Okay now we can check out the fit of our anova model. We use summary to print the model statistics.
```{r}
summary(anova)
```
The null hypothesis for the anova is the average points for wine is the same for wines from all five countries. That is:  
  
  
$H_0 : \mu_{Spain} = \mu_{Portugal}= \mu_{Italy} = \mu_{US} = \mu_{France}$  
  
Our anova results rejects this null, so we know **at least one** of the means is significantly different. Let's do run Tukey's post hoc to see how the groups vary. We use the *TukeyHSD()* and put the variable we named our anova inside.   
```{r}
TukeyHSD(anova)
```
This is kind of hard to work. Luckily the tidyverse has a package called broom (we loaded this in earlier) and broom has a function called *tidy()*
```{r}
Tukeys<-TukeyHSD(anova)%>%tidy()

#Let's highlight significant differences
Tukeys<-Tukeys %>% mutate(sig = case_when(adj.p.value < .05~ '*', TRUE ~''))
Tukeys
```
Let's plot this now. We can use summarize to get our means, se, and then add a column with our Tukey groupings 
```{r}
country_summary <-country_df %>% 
  group_by(country) %>%
  summarise(avg_rating=mean(points), se = sd(points)/ (sqrt(n())))%>%
  mutate(TukeyGroup = c('A', 'B', 'C', 'D', 'B'))

#Then we make the plot
plot<-ggplot(country_summary, aes(x=country, y=avg_rating, color=country))+
        geom_point(size=2)+
        geom_text(aes(label=TukeyGroup), nudge_y=.1)+
        xlab('Country')+
        ylab('Average rating for wine')+
        geom_errorbar(aes(ymin=avg_rating-se, ymax=avg_rating+se), width=.2)+
        theme_classic()+
        theme(legend.position = 'NA')
#Here it is! 
plot
```
If you want to change the colors of your graph you can set the colors: 
```{r}
plot+scale_color_manual(values=c('blue','orange', 'green', 'black', 'magenta')) # can also use hex# 
```

The package ggsci has some color palettes you may want to try! Uncomment the code
```{r}
#install.packages('ggsci')
#library(ggsci)

#plot+scale_color_lancet()
```
[Check out more ggsci options](https://cran.r-project.org/web/packages/ggsci/vignettes/ggsci.html)

Like your graph and want to save it?
```{r}
#ggsave('CountryWines.pdf') #it will save the last plot you made by default under the name & with the extension in quotes
```

Real quick other kinds of ANOVA, to test for an interaction
```{r eval=FALSE}
anova_interaction<-aov(points~country*variety, data=country_df)
summary(anova_price)
TukeyHSD(anova_price)
```


**Note on Kruskal Wallis**
If your data does not meet the assumptions for normality to run an anova you may want ot run a kruskal-wallis test you can do that in R with the function *kruskal.test()* the arguments inside are similar to the anova where you would have a formula 'y~x'

### Good resources to check out for more variations/details:
* [R cookbook](https://search.library.brandeis.edu/permalink/f/1skfba6/TN_sbo_s9780596809287) by Paul Teetor
* [Learning Statistics with R](https://learningstatisticswithr.com/book/) by Danielle Navarro

### What to do now?

Practice wrangling and running stats on the titanic data set. Some questions you could answer:
  - Is there a significant difference in age of the people that survived vs. not? 
  - Is there a significant difference in age across passenger classes?
```{r}
#Load this package
#install.packages('titanic')
#library(titanic)
#The package has a dataframe called Titanic that is now available for you to access 
#View(Titanic)
```




[This week's TidyTuesday](https://github.com/rfordatascience/tidytuesday/tree/master/data/2019/2019-06-25) 
```{r}
ufo_sightings <- readr::read_csv("https://raw.githubusercontent.com/rfordatascience/tidytuesday/master/data/2019/2019-06-25/ufo_sightings.csv")

```

